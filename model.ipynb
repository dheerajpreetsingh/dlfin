{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "df=pd.read_csv('/content/final_imputed_data.csv')\n",
        "df1=pd.read_csv('/content/df_u.csv')\n",
        "# Ensure the key combination is unique per row\n",
        "df_keys = set(zip(df['Company'], df['Year']))\n",
        "df1_keys = set(zip(df1['Company'], df1['Year']))\n",
        "\n",
        "# Find the combinations in df1 but not in df\n",
        "missing_keys = df1_keys - df_keys\n",
        "\n",
        "# Filter df1 rows that are missing in df\n",
        "missing_rows = df1[df1.apply(lambda row: (row['Company'], row['Year']) in missing_keys, axis=1)]\n",
        "\n",
        "# Concatenate to create the final dataframe\n",
        "df_final = pd.concat([df, missing_rows], ignore_index=True)\n",
        "target_cols = ['Target 1', 'Target 2', 'Target 3']\n",
        "\n",
        "for col in target_cols:\n",
        "    df_final[col] = pd.to_numeric(df_final[col], errors='coerce')\n",
        "df_final = df_final.drop(\n",
        "    df_final[\n",
        "        (df_final['Year'] < 2022) &\n",
        "        (df_final[['Target 2', 'Target 3']].isna().all(axis=1))\n",
        "    ].index\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df_final['Company_encoded'] = le.fit_transform(df_final['Company'])"
      ],
      "metadata": {
        "id": "bPUJUDebEkpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK41IQl2Ch3Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---------- Device Setup ----------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ---------- Configuration ----------\n",
        "feature_cols = [f\"Feature{i}\" for i in range(1, 29)]\n",
        "batch_size   = 1    # one company sequence per batch\n",
        "epochs       = 51\n",
        "patience     = 8\n",
        "TRAIN_SPLIT_RATIO = 0.8\n",
        "\n",
        "# ---------- DataFrames for Targets ----------\n",
        "df_target1 = df_final[df_final['Target 1'].notna()][\n",
        "    feature_cols + ['Target 1', 'Year', 'Company_encoded', 'Sector']\n",
        "].copy()\n",
        "df_target2 = df_final[df_final['Target 2'].notna()][\n",
        "    feature_cols + ['Target 2', 'Year', 'Company_encoded', 'Sector', 'Target 1']\n",
        "].copy()\n",
        "df_target3 = df_final[df_final['Target 3'].notna()][\n",
        "    feature_cols + ['Target 3', 'Year', 'Company_encoded', 'Sector', 'Target 1', 'Target 2']\n",
        "].copy()\n",
        "\n",
        "# ---------- Sector Embeddings ----------\n",
        "def get_sector_embeddings(df):\n",
        "    st = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    sectors = df['Sector'].fillna('Unknown').unique()\n",
        "    embs = st.encode(sectors, show_progress_bar=True).astype(np.float32)\n",
        "    mapping = dict(zip(sectors, embs))\n",
        "    df['Sector_Emb'] = df['Sector'].map(mapping)\n",
        "    return df\n",
        "\n",
        "# ---------- Attention Pooling ----------\n",
        "class AttentionPooling(nn.Module):\n",
        "    def _init_(self, dim):\n",
        "        super()._init_()\n",
        "        self.q = nn.Parameter(torch.randn(dim))\n",
        "        self.lin = nn.Linear(dim, dim)\n",
        "    def forward(self, x):       # x: (B, T, D)\n",
        "        p = torch.tanh(self.lin(x))                  # (B,T,D)\n",
        "        q = self.q.view(1,1,-1)                      # (1,1,D)\n",
        "        scores = (p * q).sum(-1, keepdim=True)       # (B,T,1)\n",
        "        weights = torch.softmax(scores, dim=1)       # (B,T,1)\n",
        "        return (x * weights).sum(dim=1)              # (B,D)\n",
        "\n",
        "# ---------- Convolutional Stem + SE Block ----------\n",
        "class SqueezeExcite(nn.Module):\n",
        "    def _init_(self, channels, reduction=8):\n",
        "        super()._init_()\n",
        "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
        "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
        "    def forward(self, x):  # x: (B, C, T)\n",
        "        s = x.mean(-1)               # (B, C)\n",
        "        s = torch.relu(self.fc1(s))\n",
        "        s = torch.sigmoid(self.fc2(s))\n",
        "        return x * s.unsqueeze(-1)   # (B,C,T)\n",
        "\n",
        "class ConvStem(nn.Module):\n",
        "    def _init_(self, in_channels, out_channels, kernel_size=3, depth=3):\n",
        "        super()._init_()\n",
        "        layers = []\n",
        "        for i in range(depth):\n",
        "            inc = in_channels if i==0 else out_channels\n",
        "            layers += [\n",
        "                nn.Conv1d(inc, out_channels, kernel_size, padding=kernel_size//2),\n",
        "                nn.BatchNorm1d(out_channels),\n",
        "                nn.ReLU(),\n",
        "                SqueezeExcite(out_channels),\n",
        "                nn.Dropout(0.2)\n",
        "            ]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x):    # x: (B, T, F)\n",
        "        x = x.transpose(1,2) # (B, F, T)\n",
        "        return self.net(x).transpose(1,2)  # (B, T, C)\n",
        "\n",
        "# ---------- Improved Hybrid Model ----------\n",
        "class HybridModel(nn.Module):\n",
        "    def _init_(self, feat_dim, emb_dim, d_model=128, heads=4, layers=2):\n",
        "        super()._init_()\n",
        "        # Convolutional stem\n",
        "        self.stem = ConvStem(feat_dim, out_channels=64, depth=2)\n",
        "        # Feature projection to d_model\n",
        "        self.feat_proj = nn.Linear(64, d_model)\n",
        "        # Year embedding\n",
        "        self.year_proj = nn.Linear(1, d_model)\n",
        "        # Sector projection\n",
        "        self.sec_proj  = nn.Linear(emb_dim, d_model)\n",
        "        # Positional encoding\n",
        "        self.pos_enc   = nn.Parameter(torch.randn(1, 100, d_model))\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=heads,\n",
        "            dim_feedforward=256,\n",
        "            dropout=0.2,\n",
        "            batch_first=True,\n",
        "            activation='gelu'\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=layers)\n",
        "        # Cross-attention between meta and features\n",
        "        self.cross_attn  = nn.MultiheadAttention(d_model, heads, batch_first=True, dropout=0.2)\n",
        "        self.norm        = nn.LayerNorm(d_model)\n",
        "        # Attention pooling + MLP head\n",
        "        self.pool        = AttentionPooling(d_model)\n",
        "        self.mlp         = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model//2),\n",
        "            nn.LayerNorm(d_model//2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(d_model//2, 1)\n",
        "        )\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2.0))\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x, year, sector, extra=None):\n",
        "        # x: (B,T,F); year: (B,T,1); sector: (B,T,emb_dim)\n",
        "        B,T,_ = x.shape\n",
        "        # Conv stem\n",
        "        h = self.stem(x)                  # (B,T,64)\n",
        "        h = self.feat_proj(h)             # (B,T,d_model)\n",
        "        # Meta\n",
        "        ye = self.year_proj(year)         # (B,T,d_model)\n",
        "        se = self.sec_proj(sector)        # (B,T,d_model)\n",
        "        # Positional + sum\n",
        "        h = h + ye + se + self.pos_enc[:,:T,:]\n",
        "        # Transformer encoding\n",
        "        h = self.transformer(h)           # (B,T,d_model)\n",
        "        # Cross-attention: meta as query\n",
        "        meta = ye + se                    # (B,T,d_model)\n",
        "        attn_out, _ = self.cross_attn(meta, h, h)\n",
        "        h = self.norm(h + attn_out)\n",
        "        # Pool and output\n",
        "        pooled = self.pool(h)             # (B,d_model)\n",
        "        return self.mlp(pooled)           # (B,1)\n",
        "\n",
        "# ---------- Sequence Preparation ----------\n",
        "def prepare_sequence(df, target_col):\n",
        "    df = df.sort_values('Year')\n",
        "    X = df[feature_cols].values.astype(np.float32)\n",
        "    if target_col == 'Target 2':\n",
        "        X = np.hstack([X, df['Target 1'].values.reshape(-1,1).astype(np.float32)])\n",
        "    elif target_col == 'Target 3':\n",
        "        extras = df[['Target 1','Target 2']].values.astype(np.float32)\n",
        "        X = np.hstack([X, extras])\n",
        "    X   = torch.tensor(X, device=device).unsqueeze(0)\n",
        "    year= torch.tensor(df['Year'].values.astype(np.float32).reshape(1,-1,1),\n",
        "                       device=device)\n",
        "    sec = torch.tensor(np.stack(df['Sector_Emb'].values).astype(np.float32),\n",
        "                       device=device).unsqueeze(0)\n",
        "    y   = torch.tensor(df[target_col].values.astype(np.float32)[-1],\n",
        "                       device=device).view(1,1)\n",
        "    return DataLoader(TensorDataset(X, year, sec, y),\n",
        "                      batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ---------- Early Stopping ----------\n",
        "class EarlyStopping:\n",
        "    def _init_(self, patience=patience, delta=1e-4, path='chkpt.pt'):\n",
        "        self.patience,self.delta,self.path = patience,delta,path\n",
        "        self.best,self.counter,self.stop = None,0,False\n",
        "    def _call_(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        if self.best is None or score > self.best + self.delta:\n",
        "            self.best = score\n",
        "            torch.save(model.state_dict(), self.path)\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.stop = True\n",
        "\n",
        "# ---------- Training Function ----------\n",
        "def train_model_fn(model, train_loader, val_loader):\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, factor=0.5, patience=3, min_lr=1e-6\n",
        "    )\n",
        "    criterion = nn.MSELoss()\n",
        "    stopper   = EarlyStopping()\n",
        "    history   = {'tr':[], 'va':[]}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        tr_losses = []\n",
        "        for X,yr,sec,y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(X, yr, sec)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            tr_losses.append(loss.item())\n",
        "        tr_rmse = math.sqrt(np.mean(tr_losses)) if tr_losses else float('nan')\n",
        "\n",
        "        model.eval()\n",
        "        va_losses = []\n",
        "        with torch.no_grad():\n",
        "            for X,yr,sec,y in val_loader:\n",
        "                va_losses.append(criterion(model(X, yr, sec), y).item())\n",
        "        va_rmse = math.sqrt(np.mean(va_losses)) if va_losses else float('nan')\n",
        "\n",
        "        scheduler.step(va_rmse)\n",
        "        history['tr'].append(tr_rmse)\n",
        "        history['va'].append(va_rmse)\n",
        "\n",
        "        stopper(va_rmse, model)\n",
        "        if stopper.stop:\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(torch.load(stopper.path))\n",
        "    return model, history\n",
        "\n",
        "# ---------- Train Target 3 (example) ----------\n",
        "'''df3 = get_sector_embeddings(df_target3)\n",
        "companies3 = df3['Company_encoded'].unique()\n",
        "tot_tr = tot_va = tot_ep = cnt = skip = 0\n",
        "pbar = tqdm(companies3, desc=\"Training Target 3\")\n",
        "\n",
        "for cid in pbar:\n",
        "    sub = df3[df3['Company_encoded'] == cid]\n",
        "    years = sorted(sub['Year'].unique())\n",
        "    split = int(TRAIN_SPLIT_RATIO * len(years))\n",
        "    tr_df = sub[sub['Year'].isin(years[:split])]\n",
        "    va_df = sub[sub['Year'].isin(years[split:])]\n",
        "\n",
        "    if len(tr_df) < 3 or len(va_df) < 2:\n",
        "        skip += 1\n",
        "        pbar.update(1)\n",
        "        continue\n",
        "\n",
        "    tl = prepare_sequence(tr_df, 'Target 3')\n",
        "    vl = prepare_sequence(va_df, 'Target 3')\n",
        "    model3 = HybridModel(\n",
        "        feat_dim=tl.dataset.tensors[0].shape[-1],\n",
        "        emb_dim=tl.dataset.tensors[2].shape[-1],\n",
        "        d_model=128, heads=4, layers=2\n",
        "    ).to(device)\n",
        "\n",
        "    model3, hist3 = train_model_fn(model3, tl, vl)\n",
        "    tr_rm, va_rm, ep = hist3['tr'][-1], hist3['va'][-1], len(hist3['va'])\n",
        "\n",
        "    tot_tr += tr_rm; tot_va += va_rm; tot_ep += ep; cnt += 1\n",
        "    pbar.set_postfix({\n",
        "        'avg_tr': f\"{tot_tr/cnt:.4f}\",\n",
        "        'avg_va': f\"{tot_va/cnt:.4f}\",\n",
        "        'avg_ep': f\"{tot_ep/cnt:.1f}\"\n",
        "    })\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar.close()\n",
        "print(f\"Target 3 ▶ avg_train={tot_tr/cnt:.4f}, avg_val={tot_va/cnt:.4f}, skipped={skip}\")'''\n",
        "df1 = get_sector_embeddings(df_target1)\n",
        "companies1 = df1['Company_encoded'].unique()\n",
        "tot_tr1 = tot_va1 = tot_ep1 = cnt1 = skip1 = 0\n",
        "pbar1 = tqdm(companies1, desc=\"Training Target 1\")\n",
        "for cid in pbar1:\n",
        "    sub = df1[df1['Company_encoded'] == cid]\n",
        "    years = sorted(sub['Year'].unique())\n",
        "    split = int(0.8 * len(years))\n",
        "    tr_df = sub[sub['Year'].isin(years[:split])]\n",
        "    va_df = sub[sub['Year'].isin(years[split:])]\n",
        "    if len(tr_df) < 3 or len(va_df) < 2:\n",
        "        skip1 += 1\n",
        "        continue\n",
        "    tl = prepare_sequence(tr_df, 'Target 1')\n",
        "    vl = prepare_sequence(va_df, 'Target 1')\n",
        "    model1 = BalancedModel(\n",
        "        input_dim=tl.dataset.tensors[0].shape[-1],\n",
        "        meta_dim=tl.dataset.tensors[2].shape[-1]\n",
        "    ).to(device)\n",
        "    model1, hist1 = train_model_fn(model1, tl, vl)\n",
        "    tr_rm1, va_rm1, ep1 = hist1['tr'][-1], hist1['va'][-1], len(hist1['va'])\n",
        "    tot_tr1 += tr_rm1; tot_va1 += va_rm1; tot_ep1 += ep1; cnt1 += 1\n",
        "    pbar1.set_postfix(\n",
        "        avg_tr=f\"{tot_tr1/cnt1:.4f}\", avg_va=f\"{tot_va1/cnt1:.4f}\", avg_ep=f\"{tot_ep1/cnt1:.1f}\"\n",
        "    )\n",
        "pbar1.close()\n",
        "print(f\"Target 1 ▶ avg_train={tot_tr1/cnt1:.4f}, avg_val={tot_va1/cnt1:.4f}, skipped={skip1}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---------- Device Setup ----------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ---------- Configuration ----------\n",
        "feature_cols = [f\"Feature{i}\" for i in range(1, 29)]\n",
        "batch_size = 1    # one company sequence per batch\n",
        "epochs = 51\n",
        "patience = 8      # Keeping original patience\n",
        "\n",
        "# ---------- Set Default Tensor Type ----------\n",
        "# This ensures all tensors use the same dtype\n",
        "torch.set_default_dtype(torch.float32)\n",
        "\n",
        "# ---------- DataFrames for Targets ----------\n",
        "df_target1 = df_final[df_final['Target 1'].notna()][\n",
        "    feature_cols + ['Target 1', 'Year', 'Company_encoded', 'Sector']\n",
        "].copy()\n",
        "df_target2 = df_final[df_final['Target 2'].notna()][\n",
        "    feature_cols + ['Target 2', 'Year', 'Company_encoded', 'Sector', 'Target 1']\n",
        "].copy()\n",
        "df_target3 = df_final[df_final['Target 3'].notna()][\n",
        "    feature_cols + ['Target 3', 'Year', 'Company_encoded', 'Sector', 'Target 1', 'Target 2']\n",
        "].copy()\n",
        "\n",
        "# ---------- Sector Embeddings ----------\n",
        "def get_sector_embeddings(df):\n",
        "    st = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    sectors = df['Sector'].fillna('Unknown').unique()\n",
        "    embs = st.encode(sectors, show_progress_bar=True)\n",
        "    # Ensure embeddings are float32\n",
        "    embs = embs.astype(np.float32)\n",
        "    mapping = dict(zip(sectors, embs))\n",
        "    df['Sector_Emb'] = df['Sector'].map(mapping)\n",
        "    return df\n",
        "\n",
        "# ---------- Model Architecture (Balanced) ----------\n",
        "class Time2Vec(nn.Module):\n",
        "    def _init_(self, dim):\n",
        "        super()._init_()\n",
        "        self.w0 = nn.Parameter(torch.randn(1, dtype=torch.float32))\n",
        "        self.b0 = nn.Parameter(torch.randn(1, dtype=torch.float32))\n",
        "        self.w  = nn.Parameter(torch.randn(dim-1, dtype=torch.float32))\n",
        "        self.b  = nn.Parameter(torch.randn(dim-1, dtype=torch.float32))\n",
        "    def forward(self, t):\n",
        "        # Ensure input is float32\n",
        "        t = t.to(torch.float32)\n",
        "        v0 = self.w0 * t + self.b0\n",
        "        v  = torch.sin(self.w * t + self.b)\n",
        "        return torch.cat([v0, v], dim=-1)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def _init_(self, d_model, max_len=500):\n",
        "        super()._init_()\n",
        "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
        "        pos = torch.arange(max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) *\n",
        "                        -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        # Ensure input is float32\n",
        "        x = x.to(torch.float32)\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class TransformerBranch(nn.Module):\n",
        "    def _init_(self, input_dim, meta_dim, d_model=96, heads=6, layers=2):\n",
        "        super()._init_()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        self.time2vec   = Time2Vec(d_model)\n",
        "        self.pos_enc    = PositionalEncoding(d_model)\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model, heads, dim_feedforward=192,\n",
        "            dropout=0.15,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder    = nn.TransformerEncoder(enc_layer, layers)\n",
        "        self.meta_proj  = nn.Linear(meta_dim, d_model)\n",
        "        self.cross_attn = nn.MultiheadAttention(d_model, heads, batch_first=True)\n",
        "        self.pool       = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Apply weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight, gain=0.8)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, time, meta):\n",
        "        # Ensure all inputs are float32\n",
        "        x = x.to(torch.float32)\n",
        "        time = time.to(torch.float32)\n",
        "        meta = meta.to(torch.float32)\n",
        "\n",
        "        h = self.input_proj(x)\n",
        "        h = h + self.time2vec(time)\n",
        "        h = self.pos_enc(h)\n",
        "        h = self.encoder(h)\n",
        "        m = self.meta_proj(meta)\n",
        "        attn_out, _ = self.cross_attn(m, h, h)\n",
        "        seq = h + attn_out\n",
        "        seq = seq.transpose(1, 2)  # (B, d_model, T)\n",
        "        return self.pool(seq).squeeze(-1)\n",
        "\n",
        "class SimplifiedTCN(nn.Module):\n",
        "    def _init_(self, input_dim, d_model=96):\n",
        "        super()._init_()\n",
        "        self.conv1 = nn.Conv1d(input_dim, d_model, 3, padding=2, dilation=1)\n",
        "        self.conv2 = nn.Conv1d(d_model, d_model, 3, padding=4, dilation=2)\n",
        "        self.norm  = nn.LayerNorm(d_model)\n",
        "        self.act   = nn.GELU()\n",
        "        self.pool  = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "    def forward(self, x):\n",
        "        # Ensure input is float32\n",
        "        x = x.to(torch.float32)\n",
        "        h = x.transpose(1, 2)\n",
        "        h = self.dropout(self.act(self.conv1(h)))\n",
        "        h = self.act(self.conv2(h))\n",
        "        h = h.transpose(1, 2)\n",
        "        h = self.norm(h)\n",
        "        return self.pool(h.transpose(1, 2)).squeeze(-1)\n",
        "\n",
        "class BalancedModel(nn.Module):\n",
        "    def _init_(self, input_dim, meta_dim, d_model=96):\n",
        "        super()._init_()\n",
        "        self.trans = TransformerBranch(input_dim, meta_dim, d_model)\n",
        "        self.tcn   = SimplifiedTCN(input_dim, d_model)\n",
        "        self.fc    = nn.Sequential(\n",
        "            nn.Linear(2*d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(d_model, 1)\n",
        "        )\n",
        "    def forward(self, x, time, meta):\n",
        "        h1 = self.trans(x, time, meta)\n",
        "        h2 = self.tcn(x)\n",
        "        return self.fc(torch.cat([h1, h2], dim=-1))\n",
        "\n",
        "# ---------- Sequence Preparation ----------\n",
        "def prepare_sequence(df, target_col):\n",
        "    df = df.sort_values('Year')\n",
        "    # Build feature matrix\n",
        "    X = df[feature_cols].values.astype(np.float32)  # Explicitly set dtype\n",
        "    if target_col == 'Target 2':\n",
        "        X = np.hstack([X, df['Target 1'].values.reshape(-1, 1).astype(np.float32)])\n",
        "    if target_col == 'Target 3':\n",
        "        extras = df[['Target 1', 'Target 2']].values.astype(np.float32)\n",
        "        X = np.hstack([X, extras])\n",
        "    X = torch.tensor(X, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    # Year as time input\n",
        "    year = torch.tensor(\n",
        "        df['Year'].values.astype(np.float32).reshape(1, -1, 1),\n",
        "        dtype=torch.float32, device=device\n",
        "    )\n",
        "    # Sector embedding\n",
        "    sector = torch.tensor(\n",
        "        np.stack(df['Sector_Emb'].values).astype(np.float32),\n",
        "        dtype=torch.float32, device=device\n",
        "    ).unsqueeze(0)\n",
        "    # Target: last year\n",
        "    y = torch.tensor(\n",
        "        df[target_col].values.astype(np.float32)[-1:].reshape(1, 1),\n",
        "        dtype=torch.float32, device=device\n",
        "    )\n",
        "    return DataLoader(\n",
        "        TensorDataset(X, year, sector, y),\n",
        "        batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "# ---------- Early Stopping ----------\n",
        "class EarlyStopping:\n",
        "    def _init_(self, patience=patience, delta=0.0005, path='chkpt.pt'):\n",
        "        self.patience = patience\n",
        "        self.delta    = delta\n",
        "        self.path     = path\n",
        "        self.best     = None\n",
        "        self.counter  = 0\n",
        "        self.stop     = False\n",
        "    def _call_(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        if self.best is None or score > self.best + self.delta:\n",
        "            self.best = score\n",
        "            torch.save(model.state_dict(), self.path)\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.stop = True\n",
        "\n",
        "# ---------- Training Function ----------\n",
        "def train_model_fn(model, train_loader, val_loader):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=8e-4, weight_decay=5e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.7, patience=4)\n",
        "    criterion = nn.MSELoss()\n",
        "    stopper   = EarlyStopping()\n",
        "    history   = {'tr': [], 'va': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        tr_losses = []\n",
        "        for X, yr, sec, y in train_loader:\n",
        "            # Ensure all data is float32\n",
        "            X = X.to(torch.float32)\n",
        "            yr = yr.to(torch.float32)\n",
        "            sec = sec.to(torch.float32)\n",
        "            y = y.to(torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = model(X, yr, sec)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            tr_losses.append(loss.item())\n",
        "        tr_rmse = math.sqrt(np.mean(tr_losses)) if tr_losses else float('nan')\n",
        "\n",
        "        model.eval()\n",
        "        va_losses = []\n",
        "        with torch.no_grad():\n",
        "            for X, yr, sec, y in val_loader:\n",
        "                # Ensure all data is float32\n",
        "                X = X.to(torch.float32)\n",
        "                yr = yr.to(torch.float32)\n",
        "                sec = sec.to(torch.float32)\n",
        "                y = y.to(torch.float32)\n",
        "\n",
        "                va_losses.append(criterion(model(X, yr, sec), y).item())\n",
        "        va_rmse = math.sqrt(np.mean(va_losses)) if va_losses else float('nan')\n",
        "\n",
        "        scheduler.step(va_rmse)\n",
        "\n",
        "        history['tr'].append(tr_rmse)\n",
        "        history['va'].append(va_rmse)\n",
        "        stopper(va_rmse, model)\n",
        "        if stopper.stop:\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(torch.load(stopper.path))\n",
        "    return model, history\n",
        "\n",
        "# ---------- Training Target 1 ----------\n",
        "'''df1 = get_sector_embeddings(df_target1)\n",
        "companies1 = df1['Company_encoded'].unique()\n",
        "tot_tr1 = tot_va1 = tot_ep1 = cnt1 = skip1 = 0\n",
        "pbar1 = tqdm(companies1, desc=\"Training Target 1\")\n",
        "for cid in pbar1:\n",
        "    sub = df1[df1['Company_encoded'] == cid]\n",
        "    years = sorted(sub['Year'].unique())\n",
        "\n",
        "    # Updated splitting logic with time-based holdout\n",
        "    if len(years) < 3:  # For very small datasets\n",
        "        skip1 += 1\n",
        "        continue\n",
        "    elif len(years) <= 5:  # For small datasets\n",
        "        tr_df = sub[sub['Year'] != years[-1]]  # All but last year\n",
        "        va_df = sub[sub['Year'] == years[-1]]  # Just last year\n",
        "    else:  # Normal split for larger datasets\n",
        "        split = int(0.8 * len(years))\n",
        "        tr_df = sub[sub['Year'].isin(years[:split])]\n",
        "        va_df = sub[sub['Year'].isin(years[split:])]\n",
        "\n",
        "    tl = prepare_sequence(tr_df, 'Target 1')\n",
        "    vl = prepare_sequence(va_df, 'Target 1')\n",
        "    model1 = BalancedModel(\n",
        "        input_dim=tl.dataset.tensors[0].shape[-1],\n",
        "        meta_dim=tl.dataset.tensors[2].shape[-1]\n",
        "    ).to(device)\n",
        "    model1, hist1 = train_model_fn(model1, tl, vl)\n",
        "    tr_rm1, va_rm1, ep1 = hist1['tr'][-1], hist1['va'][-1], len(hist1['va'])\n",
        "    tot_tr1 += tr_rm1; tot_va1 += va_rm1; tot_ep1 += ep1; cnt1 += 1\n",
        "    pbar1.set_postfix(\n",
        "        avg_tr=f\"{tot_tr1/cnt1:.4f}\", avg_va=f\"{tot_va1/cnt1:.4f}\", avg_ep=f\"{tot_ep1/cnt1:.1f}\"\n",
        "    )\n",
        "pbar1.close()\n",
        "print(f\"Target 1 ▶ avg_train={tot_tr1/cnt1:.4f}, avg_val={tot_va1/cnt1:.4f}, skipped={skip1}\")\n",
        "'''\n",
        "# ---------- Training Target 2 ----------\n",
        "df2 = get_sector_embeddings(df_target2)\n",
        "companies2 = df2['Company_encoded'].unique()\n",
        "tot_tr2 = tot_va2 = tot_ep2 = cnt2 = skip2 = 0\n",
        "pbar2 = tqdm(companies2, desc=\"Training Target 2\")\n",
        "for cid in pbar2:\n",
        "    sub = df2[df2['Company_encoded'] == cid]\n",
        "    years = sorted(sub['Year'].unique())\n",
        "\n",
        "    # Updated splitting logic with time-based holdout\n",
        "    if len(years) < 3:  # For very small datasets\n",
        "        skip2 += 1\n",
        "        continue\n",
        "    elif len(years) <= 5:  # For small datasets\n",
        "        tr_df = sub[sub['Year'] != years[-1]]  # All but last year\n",
        "        va_df = sub[sub['Year'] == years[-1]]  # Just last year\n",
        "    else:  # Normal split for larger datasets\n",
        "        split = int(0.8 * len(years))\n",
        "        tr_df = sub[sub['Year'].isin(years[:split])]\n",
        "        va_df = sub[sub['Year'].isin(years[split:])]\n",
        "\n",
        "    tl = prepare_sequence(tr_df, 'Target 2')\n",
        "    vl = prepare_sequence(va_df, 'Target 2')\n",
        "    model2 = BalancedModel(\n",
        "        input_dim=tl.dataset.tensors[0].shape[-1],\n",
        "        meta_dim=tl.dataset.tensors[2].shape[-1]\n",
        "    ).to(device)\n",
        "    model2, hist2 = train_model_fn(model2, tl, vl)\n",
        "    tr_rm2, va_rm2, ep2 = hist2['tr'][-1], hist2['va'][-1], len(hist2['va'])\n",
        "    tot_tr2 += tr_rm2; tot_va2 += va_rm2; tot_ep2 += ep2; cnt2 += 1\n",
        "    pbar2.set_postfix(\n",
        "        avg_tr=f\"{tot_tr2/cnt2:.4f}\", avg_va=f\"{tot_va2/cnt2:.4f}\", avg_ep=f\"{tot_ep2/cnt2:.1f}\"\n",
        "    )\n",
        "pbar2.close()\n",
        "print(f\"Target 2 ▶ avg_train={tot_tr2/cnt2:.4f}, avg_val={tot_va2/cnt2:.4f}, skipped={skip2}\")\n",
        "torch.save({\n",
        "    'model_state_dict': model2.state_dict(),\n",
        "    'input_dim': tl.dataset.tensors[0].shape[-1],\n",
        "    'meta_dim': tl.dataset.tensors[2].shape[-1],\n",
        "}, 'model2_trained.pt')\n",
        "\n",
        "# ---------- Training Target 3 ----------\n",
        "'''df3 = get_sector_embeddings(df_target3)\n",
        "companies3 = df3['Company_encoded'].unique()\n",
        "tot_tr3 = tot_va3 = tot_ep3 = cnt3 = skip3 = 0\n",
        "pbar3 = tqdm(companies3, desc=\"Training Target 3\")\n",
        "for cid in pbar3:\n",
        "    sub = df3[df3['Company_encoded'] == cid]\n",
        "    years = sorted(sub['Year'].unique())\n",
        "\n",
        "    # Updated splitting logic with time-based holdout\n",
        "    if len(years) < 3:  # For very small datasets\n",
        "        skip3 += 1\n",
        "        continue\n",
        "    elif len(years) <= 5:  # For small datasets\n",
        "        tr_df = sub[sub['Year'] != years[-1]]  # All but last year\n",
        "        va_df = sub[sub['Year'] == years[-1]]  # Just last year\n",
        "    else:  # Normal split for larger datasets\n",
        "        split = int(0.8 * len(years))\n",
        "        tr_df = sub[sub['Year'].isin(years[:split])]\n",
        "        va_df = sub[sub['Year'].isin(years[split:])]\n",
        "\n",
        "    tl = prepare_sequence(tr_df, 'Target 3')\n",
        "    vl = prepare_sequence(va_df, 'Target 3')\n",
        "    model3 = BalancedModel(\n",
        "        input_dim=tl.dataset.tensors[0].shape[-1],\n",
        "        meta_dim=tl.dataset.tensors[2].shape[-1]\n",
        "    ).to(device)\n",
        "    model3, hist3= train_model_fn(model3, tl, vl)\n",
        "    tr_rm3, va_rm3, ep3 = hist3['tr'][-1], hist3['va'][-1], len(hist3['va'])\n",
        "    tot_tr3 += tr_rm3; tot_va3 += va_rm3; tot_ep3 += ep3; cnt3 += 1\n",
        "    pbar3.set_postfix(\n",
        "        avg_tr=f\"{tot_tr3/cnt3:.4f}\", avg_va=f\"{tot_va3/cnt3:.4f}\", avg_ep=f\"{tot_ep3/cnt3:.1f}\"\n",
        "    )\n",
        "pbar3.close()\n",
        "print(f\"Target 3 ▶ avg_train={tot_tr3/cnt3:.4f}, avg_val={tot_va3/cnt3:.4f}, skipped={skip3}\")'''"
      ],
      "metadata": {
        "id": "YD_Uflw6DcYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model 3\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---------- Device Setup ----------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ---------- Configuration ----------\n",
        "feature_cols = [f\"Feature{i}\" for i in range(1, 29)]\n",
        "batch_size = 1    # one company sequence per batch\n",
        "epochs = 51\n",
        "patience = 8      # Keeping original patience\n",
        "\n",
        "# ---------- Set Default Tensor Type ----------\n",
        "# This ensures all tensors use the same dtype\n",
        "torch.set_default_dtype(torch.float32)\n",
        "\n",
        "# ---------- DataFrames for Targets ----------\n",
        "df_target1 = df_final[df_final['Target 1'].notna()][\n",
        "    feature_cols + ['Target 1', 'Year', 'Company_encoded', 'Sector']\n",
        "].copy()\n",
        "df_target2 = df_final[df_final['Target 2'].notna()][\n",
        "    feature_cols + ['Target 2', 'Year', 'Company_encoded', 'Sector', 'Target 1']\n",
        "].copy()\n",
        "df_target3 = df_final[df_final['Target 3'].notna()][\n",
        "    feature_cols + ['Target 3', 'Year', 'Company_encoded', 'Sector', 'Target 1', 'Target 2']\n",
        "].copy()\n",
        "\n",
        "# ---------- Sector Embeddings ----------\n",
        "def get_sector_embeddings(df):\n",
        "    st = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    sectors = df['Sector'].fillna('Unknown').unique()\n",
        "    embs = st.encode(sectors, show_progress_bar=True)\n",
        "    # Ensure embeddings are float32\n",
        "    embs = embs.astype(np.float32)\n",
        "    mapping = dict(zip(sectors, embs))\n",
        "    df['Sector_Emb'] = df['Sector'].map(mapping)\n",
        "    return df\n",
        "\n",
        "# ---------- Model Architecture (Balanced) ----------\n",
        "class Time2Vec(nn.Module):\n",
        "    def _init_(self, dim):\n",
        "        super()._init_()\n",
        "        self.w0 = nn.Parameter(torch.randn(1, dtype=torch.float32))\n",
        "        self.b0 = nn.Parameter(torch.randn(1, dtype=torch.float32))\n",
        "        self.w  = nn.Parameter(torch.randn(dim-1, dtype=torch.float32))\n",
        "        self.b  = nn.Parameter(torch.randn(dim-1, dtype=torch.float32))\n",
        "    def forward(self, t):\n",
        "        # Ensure input is float32\n",
        "        t = t.to(torch.float32)\n",
        "        v0 = self.w0 * t + self.b0\n",
        "        v  = torch.sin(self.w * t + self.b)\n",
        "        return torch.cat([v0, v], dim=-1)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def _init_(self, d_model, max_len=500):\n",
        "        super()._init_()\n",
        "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
        "        pos = torch.arange(max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) *\n",
        "                        -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        # Ensure input is float32\n",
        "        x = x.to(torch.float32)\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class TransformerBranch(nn.Module):\n",
        "    def _init_(self, input_dim, meta_dim, d_model=64, heads=4, layers=1, dropout=0.25):\n",
        "        super()._init_()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        self.time2vec   = Time2Vec(d_model)\n",
        "        self.pos_enc    = PositionalEncoding(d_model)\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model, heads,\n",
        "            dim_feedforward=128,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder    = nn.TransformerEncoder(enc_layer, layers)\n",
        "        self.meta_proj  = nn.Linear(meta_dim, d_model)\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "            d_model, heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.norm1      = nn.LayerNorm(d_model)\n",
        "        self.drop1      = nn.Dropout(dropout)\n",
        "        self.pool       = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # now self._init_weights is a real method\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight, gain=0.8)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, time, meta):\n",
        "        x = x.to(torch.float32)\n",
        "        time = time.to(torch.float32)\n",
        "        meta = meta.to(torch.float32)\n",
        "\n",
        "        h = self.input_proj(x) + self.time2vec(time)\n",
        "        h = self.pos_enc(h)\n",
        "        h = self.encoder(h)\n",
        "\n",
        "        m = self.meta_proj(meta)\n",
        "        attn_out, _ = self.cross_attn(m, h, h)\n",
        "        seq = self.norm1(h + self.drop1(attn_out))\n",
        "\n",
        "        seq = seq.transpose(1, 2)           # (B, d_model, T)\n",
        "        return self.pool(seq).squeeze(-1)\n",
        "\n",
        "# Define the BalancedModel class that was missing in the original code\n",
        "class BalancedModel(nn.Module):\n",
        "    def _init_(self, input_dim, meta_dim, d_model=64):\n",
        "        super()._init_()\n",
        "        self.transformer = TransformerBranch(input_dim, meta_dim, d_model=d_model)\n",
        "        self.output = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x, time, meta):\n",
        "        features = self.transformer(x, time, meta)\n",
        "        return self.output(features)\n",
        "\n",
        "# ---------- Sequence Preparation ----------\n",
        "def prepare_sequence(df, target_col):\n",
        "    df = df.sort_values('Year')\n",
        "    # Build feature matrix\n",
        "    X = df[feature_cols].values.astype(np.float32)  # Explicitly set dtype\n",
        "    if target_col == 'Target 2':\n",
        "        X = np.hstack([X, df['Target 1'].values.reshape(-1, 1).astype(np.float32)])\n",
        "    if target_col == 'Target 3':\n",
        "        extras = df[['Target 1', 'Target 2']].values.astype(np.float32)\n",
        "        X = np.hstack([X, extras])\n",
        "    X = torch.tensor(X, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    # Year as time input\n",
        "    year = torch.tensor(\n",
        "        df['Year'].values.astype(np.float32).reshape(1, -1, 1),\n",
        "        dtype=torch.float32, device=device\n",
        "    )\n",
        "    # Sector embedding\n",
        "    sector = torch.tensor(\n",
        "        np.stack(df['Sector_Emb'].values).astype(np.float32),\n",
        "        dtype=torch.float32, device=device\n",
        "    ).unsqueeze(0)\n",
        "    # Target: last year\n",
        "    y = torch.tensor(\n",
        "        df[target_col].values.astype(np.float32)[-1:].reshape(1, 1),\n",
        "        dtype=torch.float32, device=device\n",
        "    )\n",
        "    return DataLoader(\n",
        "        TensorDataset(X, year, sector, y),\n",
        "        batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "# ---------- Early Stopping ----------\n",
        "class EarlyStopping:\n",
        "    def _init_(self, patience=patience, delta=0.0005, path='chkpt.pt'):\n",
        "        self.patience = patience\n",
        "        self.delta    = delta\n",
        "        self.path     = path\n",
        "        self.best     = None\n",
        "        self.counter  = 0\n",
        "        self.stop     = False\n",
        "    def _call_(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        if self.best is None or score > self.best + self.delta:\n",
        "            self.best = score\n",
        "            torch.save(model.state_dict(), self.path)\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.stop = True\n",
        "\n",
        "# ---------- Training Function ----------\n",
        "def train_model_fn(model, train_loader, val_loader):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=8e-4, weight_decay=5e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.7, patience=4)\n",
        "    criterion = nn.MSELoss()\n",
        "    stopper   = EarlyStopping()\n",
        "    history   = {'tr': [], 'va': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        tr_losses = []\n",
        "        for X, yr, sec, y in train_loader:\n",
        "            # Ensure all data is float32\n",
        "            X = X.to(torch.float32)\n",
        "            yr = yr.to(torch.float32)\n",
        "            sec = sec.to(torch.float32)\n",
        "            y = y.to(torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = model(X, yr, sec)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            tr_losses.append(loss.item())\n",
        "        tr_rmse = math.sqrt(np.mean(tr_losses)) if tr_losses else float('nan')\n",
        "\n",
        "        model.eval()\n",
        "        va_losses = []\n",
        "        with torch.no_grad():\n",
        "            for X, yr, sec, y in val_loader:\n",
        "                # Ensure all data is float32\n",
        "                X = X.to(torch.float32)\n",
        "                yr = yr.to(torch.float32)\n",
        "                sec = sec.to(torch.float32)\n",
        "                y = y.to(torch.float32)\n",
        "\n",
        "                va_losses.append(criterion(model(X, yr, sec), y).item())\n",
        "        va_rmse = math.sqrt(np.mean(va_losses)) if va_losses else float('nan')\n",
        "\n",
        "        scheduler.step(va_rmse)\n",
        "\n",
        "        history['tr'].append(tr_rmse)\n",
        "        history['va'].append(va_rmse)\n",
        "        stopper(va_rmse, model)\n",
        "        if stopper.stop:\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(torch.load(stopper.path))\n",
        "    return model, history\n",
        "\n",
        "# ---------- Training Target 1 with Time-Based Holdout Variation ----------\n",
        "'''df1 = get_sector_embeddings(df_target1)\n",
        "companies1 = df1['Company_encoded'].unique()\n",
        "tot_tr1 = tot_va1 = tot_ep1 = cnt1 = skip1 = 0\n",
        "pbar1 = tqdm(companies1, desc=\"Training Target 1\")\n",
        "for cid in pbar1:\n",
        "    sub = df1[df1['Company_encoded'] == cid]\n",
        "    years = sorted(sub['Year'].unique())\n",
        "\n",
        "    # Apply the time-based holdout variation\n",
        "    if len(years) < 3:  # For very small datasets\n",
        "        skip1 += 1\n",
        "        continue\n",
        "    elif len(years) <= 5:  # For small datasets\n",
        "        tr_df = sub[sub['Year'] != years[-1]]  # All but last year\n",
        "        va_df = sub[sub['Year'] == years[-1]]  # Just last year\n",
        "    else:  # Normal split for larger datasets\n",
        "        split = int(0.8 * len(years))\n",
        "        tr_df = sub[sub['Year'].isin(years[:split])]\n",
        "        va_df = sub[sub['Year'].isin(years[split:])]\n",
        "\n",
        "    # Ensure minimum sizes for both training and validation sets\n",
        "    if len(tr_df) < 2 or len(va_df) < 1:\n",
        "        skip1 += 1\n",
        "        continue\n",
        "\n",
        "    tl = prepare_sequence(tr_df, 'Target 1')\n",
        "    vl = prepare_sequence(va_df, 'Target 1')\n",
        "    model1 = BalancedModel(\n",
        "        input_dim=tl.dataset.tensors[0].shape[-1],\n",
        "        meta_dim=tl.dataset.tensors[2].shape[-1]\n",
        "    ).to(device)\n",
        "    model1, hist1 = train_model_fn(model1, tl, vl)\n",
        "    tr_rm1, va_rm1, ep1 = hist1['tr'][-1], hist1['va'][-1], len(hist1['va'])\n",
        "    tot_tr1 += tr_rm1; tot_va1 += va_rm1; tot_ep1 += ep1; cnt1 += 1\n",
        "    pbar1.set_postfix(\n",
        "        avg_tr=f\"{tot_tr1/cnt1:.4f}\", avg_va=f\"{tot_va1/cnt1:.4f}\", avg_ep=f\"{tot_ep1/cnt1:.1f}\"\n",
        "    )\n",
        "pbar1.close()\n",
        "print(f\"Target 1 ▶ avg_train={tot_tr1/cnt1:.4f}, avg_val={tot_va1/cnt1:.4f}, skipped={skip1}\")'''\n",
        "\n",
        "# ---------- Training Target 2 with Time-Based Holdout Variation ----------\n",
        "'''df2 = get_sector_embeddings(df_target2)\n",
        "companies2 = df2['Company_encoded'].unique()\n",
        "tot_tr2 = tot_va2 = tot_ep2 = cnt2 = skip2 = 0\n",
        "pbar2 = tqdm(companies2, desc=\"Training Target 2\")\n",
        "for cid in pbar2:\n",
        "    sub = df2[df2['Company_encoded'] == cid]\n",
        "    years = sorted(sub['Year'].unique())\n",
        "\n",
        "    # Apply the time-based holdout variation\n",
        "    if len(years) < 3:  # For very small datasets\n",
        "        skip2 += 1\n",
        "        continue\n",
        "    elif len(years) <= 5:  # For small datasets\n",
        "        tr_df = sub[sub['Year'] != years[-1]]  # All but last year\n",
        "        va_df = sub[sub['Year'] == years[-1]]  # Just last year\n",
        "    else:  # Normal split for larger datasets\n",
        "        split = int(0.8 * len(years))\n",
        "        tr_df = sub[sub['Year'].isin(years[:split])]\n",
        "        va_df = sub[sub['Year'].isin(years[split:])]\n",
        "\n",
        "    # Ensure minimum sizes for both training and validation sets\n",
        "    if len(tr_df) < 2 or len(va_df) < 1:\n",
        "        skip2 += 1\n",
        "        continue\n",
        "\n",
        "    tl = prepare_sequence(tr_df, 'Target 2')\n",
        "    vl = prepare_sequence(va_df, 'Target 2')\n",
        "    model2 = BalancedModel(\n",
        "        input_dim=tl.dataset.tensors[0].shape[-1],\n",
        "        meta_dim=tl.dataset.tensors[2].shape[-1]\n",
        "    ).to(device)\n",
        "    model2, hist2 = train_model_fn(model2, tl, vl)\n",
        "    tr_rm2, va_rm2, ep2 = hist2['tr'][-1], hist2['va'][-1], len(hist2['va'])\n",
        "    tot_tr2 += tr_rm2; tot_va2 += va_rm2; tot_ep2 += ep2; cnt2 += 1\n",
        "    pbar2.set_postfix(\n",
        "        avg_tr=f\"{tot_tr2/cnt2:.4f}\", avg_va=f\"{tot_va2/cnt2:.4f}\", avg_ep=f\"{tot_ep2/cnt2:.1f}\"\n",
        "    )\n",
        "pbar2.close()\n",
        "print(f\"Target 2 ▶ avg_train={tot_tr2/cnt2:.4f}, avg_val={tot_va2/cnt2:.4f}, skipped={skip2}\")'''\n",
        "\n",
        "# ---------- Training Target 3 with Time-Based Holdout Variation ----------\n",
        "df3 = get_sector_embeddings(df_target3)\n",
        "companies3 = df3['Company_encoded'].unique()\n",
        "tot_tr3 = tot_va3 = tot_ep3 = cnt3 = skip3 = 0\n",
        "pbar3 = tqdm(companies3, desc=\"Training Target 3\")\n",
        "for cid in pbar3:\n",
        "    sub = df3[df3['Company_encoded'] == cid]\n",
        "    years = sorted(sub['Year'].unique())\n",
        "\n",
        "    # Apply the time-based holdout variation\n",
        "    if len(years) < 3:  # For very small datasets\n",
        "        skip3 += 1\n",
        "        continue\n",
        "    elif len(years) <= 5:  # For small datasets\n",
        "        tr_df = sub[sub['Year'] != years[-1]]  # All but last year\n",
        "        va_df = sub[sub['Year'] == years[-1]]  # Just last year\n",
        "    else:  # Normal split for larger datasets\n",
        "        split = int(0.8 * len(years))\n",
        "        tr_df = sub[sub['Year'].isin(years[:split])]\n",
        "        va_df = sub[sub['Year'].isin(years[split:])]\n",
        "\n",
        "    # Ensure minimum sizes for both training and validation sets\n",
        "    if len(tr_df) < 2 or len(va_df) < 1:\n",
        "        skip3 += 1\n",
        "        continue\n",
        "\n",
        "    tl = prepare_sequence(tr_df, 'Target 3')\n",
        "    vl = prepare_sequence(va_df, 'Target 3')\n",
        "    model3 = BalancedModel(\n",
        "        input_dim=tl.dataset.tensors[0].shape[-1],\n",
        "        meta_dim=tl.dataset.tensors[2].shape[-1]\n",
        "    ).to(device)\n",
        "    model3, hist3 = train_model_fn(model3, tl, vl)\n",
        "    tr_rm3, va_rm3, ep3 = hist3['tr'][-1], hist3['va'][-1], len(hist3['va'])\n",
        "    tot_tr3 += tr_rm3; tot_va3 += va_rm3; tot_ep3 += ep3; cnt3 += 1\n",
        "    pbar3.set_postfix(\n",
        "        avg_tr=f\"{tot_tr3/cnt3:.4f}\", avg_va=f\"{tot_va3/cnt3:.4f}\", avg_ep=f\"{tot_ep3/cnt3:.1f}\"\n",
        "    )\n",
        "pbar3.close()\n",
        "print(f\"Target 3 ▶ avg_train={tot_tr3/cnt3:.4f}, avg_val={tot_va3/cnt3:.4f}, skipped={skip3}\")\n",
        "save_path = \"model3.pth\"\n",
        "torch.save(model3.state_dict(), save_path)\n",
        "print(f\"Saved Target 3 model weights to {save_path}\")"
      ],
      "metadata": {
        "id": "1QvnVtNsDmfh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}